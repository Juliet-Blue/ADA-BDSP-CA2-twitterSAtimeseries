{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2BDFyAIX8yi"
   },
   "source": [
    "## **Code Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEen7z7DWoNn"
   },
   "source": [
    "**Installing Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TG3uz45dbY-"
   },
   "outputs": [],
   "source": [
    "#!pip install pathlib\n",
    "#!pip install patool\n",
    "#!pip install internetarchive\n",
    "#!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2WZg2sdWuaD"
   },
   "source": [
    "**Setup Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpfZMtIxddWU"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "# Define path name for downloaded datasets\n",
    "path_to_files = '../archive.org/'\n",
    "#path_to_files = '../archive_test/'\n",
    "\n",
    "\n",
    "# Define a list of identifiers for each dataset to be downloaded\n",
    "months_list = listdir(path_to_files)\n",
    "\n",
    "# Define index name for Elasticsearch\n",
    "#es_index_name = 'tweet_results_2017-11'\n",
    "\n",
    "# Define searchterms\n",
    "\n",
    "searchterm = ('Misha Collins', 'misha collins', 'mishacollins', 'MishaCollins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hUC3AxfpYB77"
   },
   "source": [
    "## **Code Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c56Fe8pKYT_A"
   },
   "source": [
    "**Extract Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KKnYK4J1dkkp"
   },
   "outputs": [],
   "source": [
    "# This code cell extracts in 2 steps all datasets to a predefined path: \n",
    "# 1) Extract all TAR files to get BZ2 files\n",
    "# 2) Extract all BZ2 files to get JSON files + analyze tweet text from datasets\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import patoolib\n",
    "from pathlib import Path\n",
    "\n",
    "# Unzips all tar files\n",
    "def extract_tar(path, month):\n",
    "    path_to_files = path + month\n",
    "    \n",
    "    print(\"2. Unzip TAR files ...\")\n",
    "    \n",
    "    for item in glob.glob(path_to_files + '/*.tar'): \n",
    "        dirpath = os.path.dirname(item)\n",
    "        patoolib.extract_archive(item, outdir=dirpath)\n",
    "        os.remove(os.fspath(item))\n",
    "    return\n",
    "\n",
    "# Unzips all bz2 files from the folders\n",
    "def extract_gz(path, month_path, month, searchterm):\n",
    "    path_to_files = path + month_path\n",
    "    st = searchterm\n",
    "    result_file = 'key_tweets_'+month+'.json'\n",
    "    tweet_array = []\n",
    "    \n",
    "    print(\"3. Extracting gz files ...\")\n",
    "    \n",
    "    for item in glob.glob(path_to_files + '/**/*.gz', recursive=True):\n",
    "        dirpath = os.path.dirname(item)\n",
    "        patoolib.extract_archive(item, outdir=dirpath)\n",
    "        \n",
    "        # Analyze tweets with function analyze_tweet_text()\n",
    "        tweet_array = analyze_tweet_text(path_to_files, st)\n",
    "        \n",
    "        # Write key tweets to result file \n",
    "        # can i change this to dumping straight into a mongodb DB?\n",
    "        with open(result_file, 'a', encoding=\"utf-8\") as file:\n",
    "            for it in tweet_array:\n",
    "                file.write(\"%s\\n\" % json.dumps(it))\n",
    "        \n",
    "        os.remove(os.fspath(item))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fML-zHD4Yz2q"
   },
   "source": [
    "**Analyze Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iHZcZ7qeqz8c"
   },
   "outputs": [],
   "source": [
    "# This code cell filters JSON files for tweets which contain the one of the terms in the predefined searchterm list and \n",
    "# extracts only the key attributes from relevant tweets\n",
    "# Fuction analyze_tweet_text() gets called in the function extract_bz2()\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from glob import iglob\n",
    "\n",
    "def analyze_tweet_text(path, searchterms):\n",
    "    path_to_json = path\n",
    "    st = searchterms\n",
    "    tweets_final = [] # Array for final Tweets\n",
    "    \n",
    "    print(\"4. Analyze Tweets ...\")\n",
    "\n",
    "    # Search for JSON files\n",
    "    print(\"     Importing data ...\")\n",
    "    rootdir = Path(path_to_json)\n",
    "    json_files = list(rootdir.glob('**/*.json')) # List with all JSON Files\n",
    "\n",
    "    # Start filter process\n",
    "    print(\"     Filter process started ...\")\n",
    "    for index, js in enumerate(json_files):\n",
    "        with open(js, encoding='utf-8') as json_file:\n",
    "        #with open(os.path.join(path_to_json, js), encoding='utf-8') as json_file:\n",
    "            for line in json_file:\n",
    "                if line.strip():\n",
    "                    tweet_line = json.loads(line)\n",
    "\n",
    "                # 1. Filter: Check for deleted tweets\n",
    "                    if 'source' in tweet_line:\n",
    "                    #  print(\"Tweet exists\")\n",
    "\n",
    "                # 2. Filter: Check if tweet has more than 140 characters (truncated = true)\n",
    "                        if tweet_line['truncated'] == True:\n",
    "                            tweet_text = tweet_line['extended_tweet']['full_text']\n",
    "                        else:\n",
    "                            tweet_text = tweet_line['text']\n",
    "\n",
    "                # 3. Filter: Check if text contains any of the searchterms\n",
    "                        if any(s in tweet_text for s in st):\n",
    "                            key_tweet = []\n",
    "\n",
    "                            try:\n",
    "                                key_tweet.append(tweet_line)\n",
    "                                tweets_final.append(extract_key_info(key_tweet))\n",
    "                                print(len(tweets_final))\n",
    "                            except ValueError:\n",
    "                                print(\"Decoding JSON has failed\")\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "        \n",
    "\n",
    "    os.remove(os.fspath(js))\n",
    "    return tweets_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvHe8MJoYH8q"
   },
   "source": [
    "**Extract Key Information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5cFVE6BVdfvB"
   },
   "outputs": [],
   "source": [
    "# This code cell extracts only the key attributes from the key files \n",
    "# Fuction extract_key_info() gets called in the function analyze_tweet_text()\n",
    "\n",
    "# don't do this, do the processing wih the mongodb file on the VM instead?\n",
    "# only need created datetime, tweet text\n",
    "\n",
    "import json\n",
    "\n",
    "def extract_key_info(tweets):\n",
    "    raw_tweets = tweets # Array for key tweets\n",
    "    extracted_tweets = [] # Array for extracted tweets\n",
    "    \n",
    "    # Extract Date, ID, Text, User-ID, User-Name and User-Timezone\n",
    "    for i in range(len(raw_tweets)):\n",
    "        # Text Attributes - Check if text contains more than 140 characters\n",
    "        if raw_tweets[i]['truncated'] == True:\n",
    "            tweet_text = raw_tweets[i]['extended_tweet']['full_text']\n",
    "        else:\n",
    "            tweet_text = raw_tweets[i]['text']\n",
    "\n",
    "        # Tweet Key Values\n",
    "        tweet_date = raw_tweets[i]['created_at']\n",
    "        #tweet_id = raw_tweets[i]['id']\n",
    "        tweet_source = raw_tweets[i]['source']\n",
    "\n",
    "        # Tweet User Attributes\n",
    "        #tweet_user_id = raw_tweets[i]['user']['id']\n",
    "        #tweet_user_name = raw_tweets[i]['user']['name']\n",
    "        #tweet_user_location = raw_tweets[i]['user']['location']\n",
    "        #tweet_user_url =  raw_tweets[i]['user']['url']\n",
    "        #tweet_user_description = raw_tweets[i]['user']['description']\n",
    "        #tweet_user_verified = raw_tweets[i]['user']['verified']\n",
    "        #tweet_user_follower_count = raw_tweets[i]['user']['followers_count']\n",
    "        #tweet_user_friends_count = raw_tweets[i]['user']['friends_count']\n",
    "        #tweet_user_favourites_count = raw_tweets[i]['user']['favourites_count']\n",
    "        #tweet_user_statuses_count = raw_tweets[i]['user']['statuses_count']\n",
    "        #tweet_user_created_at = raw_tweets[i]['user']['created_at']\n",
    "        #tweet_user_utc_offset = raw_tweets[i]['user']['utc_offset']\n",
    "        #tweet_user_timezone = raw_tweets[i]['user']['time_zone']\n",
    "        #tweet_user_geo_enabled = raw_tweets[i]['user']['geo_enabled']\n",
    "        #tweet_user_language = raw_tweets[i]['user']['lang']\n",
    "\n",
    "        # Tweet Attributes\n",
    "        #tweet_geo = raw_tweets[i]['geo']\n",
    "        #tweet_coordinates = raw_tweets[i]['coordinates']\n",
    "        #tweet_place = raw_tweets[i]['place']\n",
    "        #tweet_quote_count = raw_tweets[i]['quote_count']\n",
    "        #tweet_reply_count = raw_tweets[i]['reply_count']\n",
    "        #tweet_retweet_count = raw_tweets[i]['retweet_count']\n",
    "        #tweet_favorite_count = raw_tweets[i]['favorite_count']\n",
    "        #tweet_hastags = raw_tweets[i]['entities']['hastags']\n",
    "        #tweet_urls = raw_tweets[i]['entities']['urls']\n",
    "        #tweet_favorited = raw_tweets[i]['favorited']\n",
    "        #tweet_retweeted = raw_tweets[i]['retweeted']\n",
    "        tweet_language = raw_tweets[i]['lang']\n",
    "        tweet_timestamp = raw_tweets[i]['timestamp_ms']\n",
    "\n",
    "        # Create a new JSON-Object structure\n",
    "        jsonobj = {\n",
    "            \"created_at\": tweet_date,\n",
    "            #\"id\": tweet_id,\n",
    "            \"text\": tweet_text,\n",
    "            \"source\": tweet_source,\n",
    "            \"\"\"\n",
    "            \"user\": {\n",
    "                \"id\": tweet_user_id,\n",
    "                \"name\": tweet_user_name,\n",
    "                \"location\": tweet_user_location,\n",
    "                \"url\": tweet_user_url,\n",
    "                \"description\": tweet_user_description,\n",
    "                \"verified\": tweet_user_verified,\n",
    "                \"followers_count\": tweet_user_follower_count,\n",
    "                \"friends_count\": tweet_user_friends_count,\n",
    "                \"favourites_count\": tweet_user_favourites_count,\n",
    "                \"statuses_count\": tweet_user_statuses_count,\n",
    "                \"created_at\": tweet_user_created_at,\n",
    "                \"utc_offset\": tweet_user_utc_offset,\n",
    "                \"time_zone\": tweet_user_timezone,\n",
    "                \"geo_enabled\": tweet_user_geo_enabled,\n",
    "                \"lang\": tweet_user_language,\n",
    "                },\n",
    "            \"\"\" \n",
    "            #\"geo\": tweet_geo,\n",
    "            #\"coordinates\": tweet_coordinates,\n",
    "            #\"place\": tweet_place,\n",
    "            #\"quote_count\": tweet_quote_count,\n",
    "            #\"reply_count\": tweet_reply_count,\n",
    "            #\"retweet_count\": tweet_retweet_count,\n",
    "            #\"favorite_count\": tweet_favorite_count,\n",
    "            #\"hastags\": tweet_hastags,\n",
    "            #\"urls\": tweet_urls,\n",
    "            #\"favorited\": tweet_favorited,\n",
    "            #\"retweeted\": tweet_retweeted,\n",
    "            \"lang\": tweet_language,\n",
    "            \"timestamp_ms\": tweet_timestamp,\n",
    "        }\n",
    "\n",
    "        extracted_tweets.append(jsonobj)\n",
    "\n",
    "    return extracted_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yb6nUz7zXpmd"
   },
   "source": [
    "## **Execute Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_list = [month[-7:] for month in months_list]\n",
    "month_dict = dict(zip(month_list, months_list)) #im really running out of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-11',\n",
       " '2021-12',\n",
       " '2022-01',\n",
       " '2022-02',\n",
       " '2022-03',\n",
       " '2022-04',\n",
       " '2022-05',\n",
       " '2022-06',\n",
       " '2022-07',\n",
       " '2022-08',\n",
       " '2022-09',\n",
       " '2022-10',\n",
       " '2022-11']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archiveteam-twitter-stream-2022-10'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month = month_list[-2] #yes i could do the whole year as a loop but i can't download that much at once so. month at a time.\n",
    "month_dict[month]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-IHn-LldrnT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Unzip TAR files ...\n",
      "patool: Extracting ../archive.org/archiveteam-twitter-stream-2022-10\\twitter-stream-20221001.tar ...\n",
      "patool: running C:\\WINDOWS\\system32\\tar.EXE --extract --file ../archive.org/archiveteam-twitter-stream-2022-10\\twitter-stream-20221001.tar --directory ../archive.org/archiveteam-twitter-stream-2022-10\n"
     ]
    }
   ],
   "source": [
    "# 1) Download datasets\n",
    "#downloader(identifier_list, path_to_files)\n",
    "\n",
    "# 2) Extract datasets to BZ2 files\n",
    "extract_tar(path_to_files, month_dict[month])\n",
    "\n",
    "# 3) Extract BZ2 files to JSON files and analyze files for relevant tweets\n",
    "extract_gz(path_to_files, month_dict[month], month, searchterm) #calls function analyze_tweet_text()\n",
    "\n",
    "# 4) Index relevant tweets to Elasticsearch\n",
    "#index_to_es(es_index_name, rf_key_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Final_BA_Code.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
